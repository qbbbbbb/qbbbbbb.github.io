<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Cache一致性</title>
    <link href="/2023/10/29/Cache%E4%B8%80%E8%87%B4%E6%80%A7/"/>
    <url>/2023/10/29/Cache%E4%B8%80%E8%87%B4%E6%80%A7/</url>
    
    <content type="html"><![CDATA[<ul><li>参考<ul><li><a href="https://zhuanlan.zhihu.com/p/136300660"class="uri">https://zhuanlan.zhihu.com/p/136300660</a></li><li><a href="https://zhuanlan.zhihu.com/p/515450647"class="uri">https://zhuanlan.zhihu.com/p/515450647</a></li></ul></li></ul><hr /><h2 id="cache-的-invalidate-clean">1. Cache 的 invalidate &amp;clean</h2><ul><li>invalidate<ul><li>将相应位置的cacheline状态置为无效，<strong>将validbit置为0</strong>. 并不需要清除相应位置的cacheline数据。</li><li>复位之后，需要将所有的cacheline的valid信号置为0，防止复位后，cache命中拿到错误未定义的数据。</li></ul></li><li>clean<ul><li>将<strong>dirty bit</strong>为1的cacheline写回主存中，同时拉低cacheline的dirtybit，通过这种方式可以将cache中的数据和主存中数据保持一致（针对写回策略）。</li></ul></li></ul><h2 id="cache-dma一致性">2. Cache &amp; DMA一致性</h2><ul><li>DMA &amp; Cache<ul><li>DMA相关内容可见<ahref="https://i.cnblogs.com/posts?cateId=2319426">这个专题</a>。DMA可以实现不经过CPU实现外设和存储器以及存储器和存储器的数据传输。</li><li><ahref="https://www.cnblogs.com/qianbinbin/p/17468440.html">这篇博客</a>中介绍Cache的位置为CPU Registers 和 主存之间数据交互的桥梁。</li><li>可能会出现如下情况：Cache采用写回策略，Cache中更改的数据暂未被写回到主存，但此时DMA已经将主存中旧的数据传输给外设接口；程序运行出现问题。</li></ul></li><li>Cache的总线监视技术<ul><li>为了解决上面Cache和DMA不一致的问题，可以在DMA通过总线获取数据时，先检查cache是否命中，如果命中的话，数据应该来自cache而不是主存。</li><li>可以通过cache的总线监视技术实现，cache控制器会监视总线上的每一条内存访问，检查是否命中，根据命中情况做下一步操作。<ul><li>DMA操作的是物理地址，cache若想监视DMA的访问地址，那么需要Cache也是按照物理地址进行查找的，所以选择PIPTCache结构。</li></ul></li><li>对于一些已经设计完成的硬件电路，并不支持总线监视技术，那么也可以通过其它方式避免DMA和Cache不一致的问题。</li></ul></li><li>No Cache<ul><li><ahref="https://www.cnblogs.com/qianbinbin/p/17491401.html">这篇博客</a>中有介绍DMA在主存中实现双缓冲用做主存到IO口/IO口到主存的数据传输，为了避免cache的影响，我们可以选择将这段内存映射为nocache。</li><li>这种方法很简单实用，但缺点是：如果偶尔使用DMA，但将缓冲区一段的内存设为nocache，导致CPU Register-主存 的性能损失。</li></ul></li><li>软件维护一致性<ul><li>为了避免NoCache带来的性能损失，可以选择映射仍采用cache的方式，根据DMA传输方向不同分情况讨论。<ul><li>DMA传输方向为：I/O -&gt; 内存(DMA Buffer)<ul><li>在DMA传输之前，可以invalidate DMA buffer段的cache。</li><li>在DMA传输完成之后，由于buffer对应段的cachelinevalid置为0，数据需要重新加载，并将valid重新置为1.这样CPU读cache中的数据就不会读到过时的数据了。</li></ul></li><li>DMA传输方向为：内存(DMA Buffer) -&gt; I/O<ul><li>在DMA传输前，可以clean DMA buffer段的cache。</li><li>将cache中dirty bit被拉高的cacheline写回主存中，这样在DMA传输时，就不会将主存中过时数据发送到I/O设备。</li></ul></li></ul></li><li>注意：在DMA传输没有完成期间，我们需要保证CPU不要访问DMABuffer。因为可能CPU读走的数据并不是最终完成DMA传输的数据，导致程序错误。</li></ul></li><li>DMA Buffer对齐<ul><li>实例可以看<ahref="https://zhuanlan.zhihu.com/p/109919756">这篇文章</a>。</li><li>简要概述一下文章中描述的情况，如下：<ul><li>若未对齐，那么变量temp和buffer在cacheline中的存放如下图所示。</li><li>现DMA进行外设-&gt;内存传输数据到buffer变量中。</li><li>但在传输过程中，DMA传输到buff[3]时，CPU对temp的进行改动；之后DMA传输到buff[50]时，由于其它操作可能需要替换掉temp所在cacheline，发现dirtybit被拉高，便需要将该行cacheline写回。这个过程都发生在DMA传输过程中。</li><li>从图中可以看到写回的时buffer[0]-buffer[59],直接覆盖掉DMA从外设传输过来的buffer值。</li><li>可以通过对齐DMAbuffer来解决这个问题，即让buffer变量不和其它数据公用一个cacheline。</li></ul><img src="3201119-20230811122740521-659669830.png" /></li></ul></li></ul><h2 id="icache-和-dcache-一致性">3. ICache 和 DCache 一致性</h2><ul><li><ahref="https://www.cnblogs.com/qianbinbin/p/17468356.html">这篇博客</a>中有介绍，在CPU的L1Cache会单独分为ICache和DCache。而L2和L3不会区分ICache和DCache。</li><li>ICache的歧义和别名<ul><li>歧义和别名在<ahref="https://www.cnblogs.com/qianbinbin/p/17470849.html">这篇博客</a>中有介绍。</li><li>歧义可以通过 Physical Tag解决，对于VIPT型Cache，仍然存在别名的问题。</li><li>考虑到ICache的Read Only特性，所以即使两个Cacheline上缓存一个物理地址上的数据也没有关系。</li></ul></li><li>不一致性的情况<ul><li>程序在执行过程中，指令一般不会修改，此时不会出现一致性问题。</li><li>但是对于一些特殊情况：self-modifyingcode，在执行时会修改自己的指令，它们修改指令的过程为：<ul><li>将需要修改的指令加载到dCache中。</li><li>修改成新指令，写回dCache。</li></ul></li><li>可能会出现的问题<ul><li>如果旧指令已经缓存在iCache中。那么对于程序执行来说依然会命中iCache。这不是我们想要的结果。</li><li>如果旧指令没有在iCache中，那么CPU会去主存中取指令数据，而如果dCache采用的是写回策略，那么指令会被写回到dCache，而不会写回主存，那么从主存中取出的指令也不是想要的。</li></ul></li></ul></li><li>不一致性的解决方案<ul><li>硬件维护<ul><li>硬件上让iCache和dCache之间通信。</li><li>每一次修改dCache数据时，去查一下iCache是否命中；如果命中，那么也更新一下iCache。</li><li>当加载指令时，先去查找iCache中是否命中，如果没有命中，再去dCache中查找。如果都没有命中再去查找主存。</li><li>但是self-modifyingcode是少数，为了解决少数的情况，却给硬件带来了很大的负担。</li></ul></li><li>软件维护<ul><li>可以通过下面的步骤维护一致性。<ul><li>（1）将需要修改的指令数据加载到dCache中。</li><li>（2）修改成新指令，写回dCache。</li><li>（3）cleandCache中修改的指令对应的cacheline，保证dCache中新指令写回主存。</li><li>（4）invalidiCache中修改的指令对应的cacheline，保证从主存中读取新指令。</li></ul></li></ul></li></ul></li></ul><h2 id="多核cache一致性">4. 多核Cache一致性</h2><ul><li>每个CPU之间都有一个L1Cache，如果为多核，需要考虑多核Cache之间的一致性。</li><li>不一致性的情况<ul><li>假设存在两个CPU，都有对应的L1Cache。首先两个CPU都读取了0x40地址数据，CPU从主存加载数据到对应的CacheLine中。</li><li>采用写回策略，CPU0写数据会更新其对应的L1_Cahce0，dirtybit被拉高。之后CPU1发现命中了L1_Cache1，将未被修改的值读出。导致数据读出错误。</li></ul></li><li>解决思路<ul><li>思路一：CPU0修改0x40的时候，除了更新CPU0的Cache之外，还应该通知CPU1的Cache更新其Cache0x40的数据。<br /></li><li>思路二：<ul><li>CPU0修改0x40的时候，除了更新CPU0的Cache之外，还可以通知CPU1的Cache将0x40地址所在cacheline置成invalid。保证CPU1读取数据时不会命中自己的Cache。</li><li>不命中CPU1的Cache之后，我们有两种选择保证读取到最新的数据。<ul><li><ol type="a"><li>从CPU0的私有cache中返回0x40的数据给CPU1；</li></ol></li><li><ol start="2" type="a"><li>CPU0发出clear信号后，将cache0 0x40的数据写回主存，CPU1cache置为invalid，从主存读取最新的数据。</li></ol></li></ul></li></ul></li></ul></li><li>解决方法<ul><li>现在几乎不会使用软件维护一致性，因为成本过高，维护一致性带来的性能损失会抵消掉一部分cache带来的性能提升。</li><li>目前主要采用硬件维护，这里介绍Bus Snooping Protocol和MESIProtocol。</li><li>Bus Snooping Protocol<ul><li>实现思路<ul><li>总线监控协议，当CPU0修改自己私有的Cache时，硬件就会广播通知到总线上其它的CPU。</li><li>对于每个CPU来说，会有特殊的硬件去监听广播事件，并检查是否有相同地址的数据被缓存在自己的CPU上。</li><li>如果其它CPU缓存上存在相同地址的数据，那么也需要对应更新cacheline。</li></ul></li><li>存在问题<ul><li>总线需要每时每刻监听总线上的一切活动，一定程度上增加了总线负载和读写延迟。</li></ul></li></ul></li><li>MESI Protocol<ul><li>主要解决针对Bus Snooping Protocol存在的问题。</li><li>MESI：主要是指四种状态，Modified、Exclusive、Shared、Invalid。</li><li>举例，继续上面的例子。<ul><li>当CPU0从主存中读取0x40的数据，将数据加载到cache0中。此时CPU1的cache1中没有该地址数据，所以可以在cache0中标记该cacheline为<strong>Exclusive</strong>状态。表示该cacheline的数据是某一个CPU独占的。</li><li>当CPU1也想读0x40的数据，CPU1就会发送消息给其它CPU，发现cache0中存在该数据，那么数据将会从cache0给到cache1.cacheline的状态变为<strong>Shared</strong>。表示该数据在多个CPU的cache中被缓存，且与主存数据保持一致。</li><li>当CPU0继续修改0x40地址的数据，发现该地址cacheline为shared状态，CPU0会发送<strong>Invalid</strong>信息给到其它CPU。<ul><li>CPU1接收到invalid信号，将对应地址(0x40)cache line置为invalid。</li><li>CPU0收到CPU1更改invalid状态的信息之后，修改0x40所在的cacheline中的数据，并更新cache line的状态为<strong>Modified</strong>。</li><li>Modified表示该cacheline数据是某一个CPU私有的，且与主存中的数据不一致，表示被修改。</li></ul></li><li>如果CPU0还想要修改cache0 0x40的数据，并发现cacheline的状态为Modified；此时CPU0不需要向其它CPU发送消息，直接更新数据就可以。因为当前cacheline数据是其CPU独有的。</li><li>如果当前cache line需要被替换，发现cacheline的状态是Modified，数据会先被写回主存中。</li></ul></li><li>上面介绍的例子中，如果cacheline的状态是Modified/Exclusive状态，修改其数据并不需要通知其它CPU，这在一定程度上减轻了带宽的压力。</li></ul></li><li>MESI Protocol Messages<ul><li>CPU、L1 Cache之间的数据和状态是通过发送message进行同步的。</li><li>主要包括以下几种Messages：<ul><li>Read: CPU需要读取某个地址的数据，发送Read Message。</li><li>Read Response: 读回复，并且返回需要读取的数据。</li><li>Invalidate: 要求其他CPU invalid对应地址的cache line。</li><li>Invalidate Acknowledge: 回复 发起invalidate的CPU，表明对应的cacheline已经被invalidate。</li><li>Read Invalidate: Read + Invalidate消息的组合。</li><li>Writeback: 该消息包含从Cache Line要回写到主存的地址和数据。</li></ul></li><li>更多关于MESI协议相关内容可见<ahref="https://www.cnblogs.com/qianbinbin/p/17731460.html">这篇博客</a>.</li></ul></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>计算机组成与设计</category>
      
      <category>Cache</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>TLB</title>
    <link href="/2023/10/29/TLB/"/>
    <url>/2023/10/29/TLB/</url>
    
    <content type="html"><![CDATA[<ul><li>参考<ul><li><a href="https://zhuanlan.zhihu.com/p/108425561"class="uri">https://zhuanlan.zhihu.com/p/108425561</a></li></ul></li></ul><hr /><h2 id="tlb由来">1. TLB由来</h2><ul><li>在<ahref="https://www.cnblogs.com/qianbinbin/p/17466384.html">这篇博客</a>中有简单介绍。</li><li>MMU根据页表，将虚拟地址映射得到对应的物理地址。<ul><li>页表在64bit系统中，常见为3-5级，以4级为例，分别有PGD。</li><li>硬件上有一个<strong>页表基地址寄存器</strong>，存放着PGD页表的首地址；之后根据虚拟地址中PGDindex查找到对应的PUD页表的首地址，依次类推，最后找到PTE中存放的物理地址。示意图在<ahref="https://zhuanlan.zhihu.com/p/108425561">这篇文章</a>中，结合看更清晰。</li></ul></li><li>上面所描述的查找过程十分费时，影响性能，考虑加一块缓存，提高速度。于是就出现了TLB。<ul><li>TLB本质上是一块高速缓存，在虚拟地址到物理地址转换时，首先查找TLB，是否命中，如果命中直接得到物理地址。如果未命中，仍需要一级一级查找页表获取物理地址，并需要将虚拟地址和物理地址的映射关系缓存到TLB中。</li></ul></li></ul><h2 id="tlb-的歧义别名">2. TLB 的歧义&amp;别名</h2><ul><li>由于TLB用途就是根据虚拟地址查找cache，所以TLB必定是VIVT。<ahref="https://www.cnblogs.com/qianbinbin/p/17470849.html">这篇博客</a>中有介绍VIVT存在<strong>歧义和别名</strong>的问题。</li></ul><h3 id="tlb-特殊处理">2.1 TLB 特殊处理</h3><ul><li>TLB 有些不同于前面学习的Cache，在<ahref="https://www.cnblogs.com/qianbinbin/p/17466384.html">这篇博客</a>中，可以看到虚拟地址映射单位为Page，一般Page大小取4KB，所以TLB不需要存储虚拟地址的低12bit，在最后得到PPN的值后直接拼接低12bits即可。</li><li>另外，命中cache之后，cache line中的数据即pageindex，所以都会被取出，不需要offset域。</li><li>index域是否存在取决于是否为全相联缓存。</li></ul><h3 id="别名">2.2 别名</h3><ul><li>VIVT 数据Cache出现别名的主要原因在<ahref="https://www.cnblogs.com/qianbinbin/p/17470849.html">这篇博客</a>。根本在于存在一个修改<strong>地址-数据映射关系</strong>，且未被同步。</li><li>TLB虽然是VIVT，但是由于其不存在修改<strong>虚拟地址-物理地址映射关系</strong>，那么就不会出现别名情况。</li></ul><h3 id="歧义">2.3 歧义</h3><ul><li>VIVT出现歧义的主要原因在于不同进程中，相同虚拟地址可能对应着不同的物理地址。</li><li>解决方法：与VIVT 数据Cache 相同，采用切换进程时flushTLB，将整个TLB置为无效；但会导致性能损失。</li></ul><h3 id="如何尽量避免flush-tlb">2.4 如何尽量避免flush TLB</h3><ul><li>前面介绍，TLB存在歧义问题主要是不同进程的相同虚拟地址对应不同物理地址，那么如果可以<strong>区分不同进程的TLB表项</strong>就可以避免flushTLB。<br /></li><li>上面的想法可以通过每行cacheline扩展几bit作为<strong>分辨进程ID</strong>（ASID:Address SpaceID），在判断TLB是否命中时，除了比较tag，还需要比较ASID，选择当前进程匹配的ASID的cacheline进行操作。</li><li>ASID的管理<ul><li>需要注意：<strong>ASID和进程ID并不是一个</strong>，进程ID取值范围很大，但ASID一般为8/16bit，只能区分256/65536个进程。所以ASID和进程ID不可能一一对应。</li><li>每创建一个新进程时，就会为之分配一个ASID，当所有的ASID都分配完了，就会flushTLB，之后重新分配。</li><li>ASID一方面需要放在TLB Cacheline中，另一方面可以将其放在前面介绍的页表基地址寄存器中拓展若干bit；在查TLB是否命中时，比较tag，以及这两部分的ASID是否相等。</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>计算机组成与设计</category>
      
      <category>Cache</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Cache-虚拟地址&amp;物理地址</title>
    <link href="/2023/10/29/Cache-%E8%99%9A%E6%8B%9F%E5%9C%B0%E5%9D%80&amp;%E7%89%A9%E7%90%86%E5%9C%B0%E5%9D%80/"/>
    <url>/2023/10/29/Cache-%E8%99%9A%E6%8B%9F%E5%9C%B0%E5%9D%80&amp;%E7%89%A9%E7%90%86%E5%9C%B0%E5%9D%80/</url>
    
    <content type="html"><![CDATA[<ul><li>参考<ul><li><a href="https://zhuanlan.zhihu.com/p/107096130"class="uri">https://zhuanlan.zhihu.com/p/107096130</a></li></ul></li></ul><hr /><h2 id="vivtvirtually-indexed-virtually-tagged">1. VIVT（VirtuallyIndexed Virtually Tagged）</h2><ul><li>虚拟高速缓存：以虚拟地址作为查找对象。</li><li>首先虚拟地址给cache，如果命中，则返回数据给cpu，如果未命中，则将虚拟地址通过MMU转化为物理地址，根据物理地址从主存中读取数据。</li><li>优点<ul><li>不需要在查找cache过程将虚拟地址翻译成物理地址，节省了MMU转换的时间，提高访问cache的访问速度。</li></ul></li><li>缺点<ul><li>引入软件使用上的问题，歧义（ambiguity）和别名（alias）。</li></ul></li></ul><h2 id="歧义">2. 歧义</h2><ul><li>歧义是指数据在cache中有相同的tag和index。</li><li>发生情况：<strong>不同进程，相同的虚拟地址映射不同的物理地址。</strong><ul><li>可能出现A进程地址a映射的数据为b，而B进程地址a映射的数据为c，当A进程运行时，访问地址a会将数据b加载到cache上，而进程B运行时，访问地址a时命中了并将数据b返回给CPU，但是实际上B进程应该把数据c给CPU。</li></ul></li><li>解决方法：切换进程，flush cache，主要有两种。<ul><li>使主存储器有效：首先将主存置为有效，将cacheline上已经修改的数据写回主存上，避免修改的数据丢失。</li><li>使高速缓存无效：首先将cacheline都置为无效，保证切换时不会有进程误命中上一进程的数据。</li></ul></li><li>所以对于VIVT来说，每次进程切换时，都可能出现大量的cache缺失，且只要切换进程就需要flushcache，导致性能的缺失。</li></ul><h2 id="别名">3. 别名</h2><ul><li>发生情况：不同虚拟地址映射到相同物理地址。<ul><li>可能出现物理地址A对应的虚拟地址为B和C，index值B&lt;C,那么当程序想要修改物理地址A对应的数据时，采用写回策略，对B进行修改，且修改的值没有同步到主存中，当程序想要访问虚拟地址C时，命中，但是取出的数据是未修改的。</li></ul></li><li>解决方法<ul><li>（1）采用nocache映射，不通过cache映射，CPU直接去主存读写数据。既适用于不同进程共享数据也适用于相同进程共享数据。</li><li>（2）不同进程共享数据，可以选择在进程切换时，flushcache，（主要因为存在一个向主存写数据的过程）。</li><li>（3）同一进程共享数据，保证虚拟地址cache大小对齐，保证每次虚拟地址都会找到同一个cacheline，下面有更详细的描述。</li></ul></li></ul><h2 id="piptphysically-indexed-physically-tagged">4. PIPT（PhysicallyIndexed Physically Tagged)</h2><ul><li>物理高速缓存：为了解决VIVT歧义和别名的问题，tag和index都取自物理地址，对于物理地址来说tag和index都是唯一的。</li><li>实现过程<ul><li>CPU发出的虚拟地址首先经过MMU转化为物理地址，给到cache控制器观察是否命中。如果未命中，将去主存中根据物理地址取出数据。</li></ul></li><li>缺点<ul><li>硬件设计较VIVT要复杂很多，需要等待MMU转换之后才可以查cache。</li><li>为了加快MMU转换速度，硬件上也会加一块TLB，虚拟地址和物理地址转换可以<ahref="https://www.cnblogs.com/qianbinbin/p/17466384.html">这篇博客</a>。</li><li>很多CPU都使用PIPT高速缓存。</li></ul></li></ul><h2 id="viptvirtually-indexed-physically-tagged">5. VIPT（VirtuallyIndexed Physically Tagged）</h2><ul><li>物理标记的虚拟高速缓存</li><li>使用虚拟index查找cacheline，与此同时，将虚拟地址给MMU转换为物理地址。MMU转换完之后cache也查找结束了，此时比对物理tag是否相同，以判断是否命中cache。</li><li>不会存在歧义<ul><li>关键在于 VIPT的tag使用的是物理地址的PFN，是唯一的。</li></ul></li><li>不会存在别名（特定情况下）<ul><li>对于大部分系统来说，虚拟地址和物理地址之间的映射最小是以页为单位，在<ahref="https://www.cnblogs.com/qianbinbin/p/17466384.html">这篇博客</a>中有介绍，页addr[11:0]内的内容无论是物理地址还是虚拟地址都是相同的。</li><li>所以对于直接映射高速缓存，如果cache的大小小于等于4KB，那么意味着虚拟地址和物理地址的index是一样的，此时VIPT和PIPT是相同的。</li><li>对于多路组相连高速缓存，每一组大小小于等于4KB，使用虚拟地址（此时等于物理地址）来做index寻址，并选择物理地址的tag来选择命中哪路的cacheline。</li></ul></li><li>别名问题<ul><li>若cache的大小大于4KB，还是会出现别名问题。</li><li>目的：避免相同物理地址的数据被加载到不同cacheline中。<ul><li>解决方法：相同物理地址数据对应的虚拟地址满足cache大小对齐。</li><li>举例：cache大小8KB = 2^13，cache line大小为256B =2^8，那么虚拟地址应该是下表几种情况，这样才会都找到第4行cacheline，不会出现别名的情况。</li><li>Linux实现中，采用该方法解决的别名问题。<style>.center {width: auto;display: table;margin-left: auto;margin-right: auto;}</style></li></ul></li></ul></li></ul><div class="center"><table><thead><tr class="header"><th>address</th></tr></thead><tbody><tr class="odd"><td>0_00100_0000_0000</td></tr><tr class="even"><td>1_00100_0000_0000</td></tr><tr class="odd"><td>10_00100_0000_0000</td></tr><tr class="even"><td>...</td></tr></tbody></table></div><ul><li>对于L1Cache的ICache，如果使用VIPT，虽然存在别名问题，但是考虑其为<strong>只读</strong>，所以即使多个虚拟地址对应一个物理地址也没有关系。</li></ul><h2 id="不存在的pivt高速缓存">6. 不存在的PIVT高速缓存</h2><ul><li>没有任何优点，首先需要通过MMU转换，消耗时间。还存在歧义和别名的问题。</li></ul><h2 id="总结">7. 总结</h2><ul><li>VIVT几乎没有人使用，软件维护成本过高。</li><li>对于多路组相连高速缓存的一路大小小于等于4KB，采用VIPT。</li><li>对于一路大于4KB的，一般采用PIPT，VIPT也可以，但是还是需要特殊处理一下别名问题。</li></ul>]]></content>
    
    
    <categories>
      
      <category>计算机组成与设计</category>
      
      <category>Cache</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Cache-分配策略&amp;更新策略</title>
    <link href="/2023/10/29/Cache-%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5&amp;%E6%9B%B4%E6%96%B0%E7%AD%96%E7%95%A5/"/>
    <url>/2023/10/29/Cache-%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5&amp;%E6%9B%B4%E6%96%B0%E7%AD%96%E7%95%A5/</url>
    
    <content type="html"><![CDATA[<ul><li>参考<ul><li><a href="https://zhuanlan.zhihu.com/p/102293437"class="uri">https://zhuanlan.zhihu.com/p/102293437</a></li><li>《计算机组成与设计 第五版》</li></ul></li></ul><hr /><h2 id="分配策略">1. 分配策略</h2><ul><li>分配策略是指什么情况需要为数据分配cache line。</li><li>读分配<ul><li>当CPU读数据时，cache缺失，分配一个cacheline来缓存从主存中取出的数据。</li></ul></li><li>写分配<ul><li>场景：CPU写数据时，cache缺失。</li><li>当不支持写分配时，写指令直接更新主存的数据。例如：对某一页内存进行初始化操作（全部写0），此时就没必要将对应数据写入cache中。</li><li>支持写分配时，会首先将主存中的数据加载到cacheline中，（变成命中），然后更新cache line的数据/主存的数据。</li></ul></li></ul><h2 id="更新策略">2. 更新策略</h2><ul><li>更新策略是指当cache命中时，写操作如何更新数据。</li><li>写直通（Write Through）<ul><li>例如，CPU执行store指令，且cache命中，首先更新cache中的数据，并更新主存中的数据。cache和主存的数据保持一致。</li><li>但是<strong>性能不佳</strong>，因为每次写操作都会引起写主存的操作，这个延时是比较大的，至少100个处理器时钟周期，大大降低处理器性能。</li><li>上面描述情况的解决方案是：<strong>write buffer</strong><ul><li>数据写入cache的同时也写入写缓冲中，之后处理器继续执行。</li><li>写缓冲不断向主存中写入数据，如果写缓冲满了，那么处理器必须停顿流水线直到写缓冲中出现空闲表项。<ul><li>我认为有点类似乒乓操作。</li></ul></li><li>如果主存写操作的速率小于处理器产生写操作的速率，那么多大容量的缓冲都没用，都会很快就满了。</li><li>即使处理器产生写操作的写速率小于主存写操作速率，也可能会产生停顿，例如出现写burst传输时，此时可以增加写缓冲容量解决。</li></ul></li></ul></li><li>写回（Write Back）<ul><li>仍是CPU执行store指令，且cache命中，我们只更新cache中的数据，而不立即写入主存，此时cache和主存的数据不一致。</li><li><strong>cache line有1个bit（dirtybit）用于记录数据是否被修改过。</strong></li><li>当cache中的数据要被替换（如：出现写失效），才会写回主存中。</li><li>需要实现一个<strong>write-back buffer</strong>。<ul><li>在出现写失效时，需要根据dirty bit判断是否要将cacheline中的数据写回到主存中。</li><li>在主存读取需要填充到cache中的数据时，将cacheline中的数据写入write-back buffer中。</li><li>之后再由buffer写入主存中。</li></ul></li></ul></li><li>写回 &amp; 写直通<ul><li>写直通的写操作可以在一个周期内完成，读取标签的同时将数据写入对应的数据块。<ul><li>如果tag匹配，那么完成写操作，处理器继续执行。</li><li>如果tag不匹配，那么产生写失效，将主存中对应地址的数据取出送到cache中。</li></ul></li><li>写回的写操作至少需要两个周期去处理：第一个周期用于判断tag是否命中，若命中则第二个周期进行cache写操作。<ul><li>不能一个周期就完成的原因：如果在第一个周期就将数据写入，如果tag没有命中，会导致cacheline原有数据未来得及写回主存中，导致数据被破坏。</li><li>也可以像写直通一样实现一个<strong>store buffer</strong>。<ul><li>第一个周期，将数据写入storebuffer中，同时查找cache，判断是否命中。</li><li>如果命中，在下一个无用的cache访问周期将新数据从buffer中写入cache。</li></ul></li></ul></li></ul></li></ul><h2 id="实例">3. 实例</h2><ul><li>具体内容可以看<ahref="https://zhuanlan.zhihu.com/p/102293437">这篇文章</a>中的实例。</li><li>这里只列出我觉得需要特殊注意的地方<ul><li>当发现cache缺失时，会出现CPU从主存中取数据给cacheline的情况，此时需要注意dirty bit是否被拉高。<ul><li>如果该bit被拉高，那么cache的数据不能直接覆盖，说明这个数据对应的地址之前发生过写回操作，并没有同步到主存中，如果直接覆盖则会丢失这部分的数据。</li><li>将被替换这部分数据写回至主存中。</li><li>将主存中0x28地址（cacheline地址对齐，这里应该要被8整除）开始的8个数据写入cacheline中，并清除掉dirty bit，将offset找到的数据返回给CPU。</li></ul></li></ul></li></ul><h2 id="cache-性能评估">4. Cache 性能评估</h2><ul><li>$ CPU时间 = (CPU执行的时钟周期数 + 等待存储访问的时钟周期数) *时钟周期 $</li><li>假设等待存储访问的时钟周期数主要来自于cache引起的，下面根据读和写分开讨论。<ul><li>读操作带来的停顿周期数 只由读失效带来。 <spanclass="math display">\[读操作带来的停顿周期数=\frac{读操作数目}{程序}*\frac{读失效次数}{指令数目}*读失效代价\]</span><br /></li><li>写操作带来的停顿周期数<ul><li>写直通策略<ul><li>有两个停顿的来源，写失效和写缓冲停顿。后者为buffer满时仍进行写操作引发的停顿。<span class="math display">\[写操作带来的停顿周期数 =\frac{写操作数目}{程序}*\frac{写失效次数}{指令数目}*写失效代价+写缓冲满时的停顿周期\]</span></li></ul></li><li>写回策略<ul><li>停顿主要来源于cache line要被替换，并需要将原数据写回主存中。</li></ul></li></ul></li></ul></li><li>前面介绍是未命中时对性能的影响，但是除了失效率，还有命中时间也会对性能有很大的影响。<ul><li>举例：增大Cache的容量（主要是Cacheline的个数），更大的Cache需要更久的命中时间。这个命中时间可以算在CPU执行的时钟周期数上。</li><li>定义 平均存储访问时间(AMAT) 将Cache的命中时间也考虑在内，公式如下。<span class="math display">\[AMAT = 命中时间+失效率*失效代价\]</span></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>计算机组成与设计</category>
      
      <category>Cache</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Cache-直接映射缓存&amp;多路组连缓存</title>
    <link href="/2023/10/29/Cache-%E7%9B%B4%E6%8E%A5%E6%98%A0%E5%B0%84%E7%BC%93%E5%AD%98&amp;%E5%A4%9A%E8%B7%AF%E7%BB%84%E8%BF%9E%E7%BC%93%E5%AD%98/"/>
    <url>/2023/10/29/Cache-%E7%9B%B4%E6%8E%A5%E6%98%A0%E5%B0%84%E7%BC%93%E5%AD%98&amp;%E5%A4%9A%E8%B7%AF%E7%BB%84%E8%BF%9E%E7%BC%93%E5%AD%98/</url>
    
    <content type="html"><![CDATA[<ul><li>参考<ul><li><a href="https://zhuanlan.zhihu.com/p/102293437"class="uri">https://zhuanlan.zhihu.com/p/102293437</a></li><li>公众号：老秦谈芯</li><li>《计算机组成与设计 第五版》 ***</li></ul></li></ul><h2 id="cache-line">1. Cache line</h2><ul><li>cachesize：这里只考虑cache可以缓存最大<strong>数据</strong>的大小。这里忽略了tag和validbit的占用。</li><li>将cache均分相等的块，每一块称为cacheline，现在的硬件设计中，一般cache line的大小为4-128字节，cacheline做的太小会导致tag资源占用过大。</li><li><strong>cache line 是 cache 和主存之间数据传输的最小单位。</strong><ul><li>考虑到程序的空间局部性：即一个数据被访问了，那么它周围的数据在之后也有可能被访问。所以cache会选择连续一段数据公用一个tag，一起被传输。</li><li>在cache缺失时，即使CPU只需要从主存中读1个字节的数据出来，但是还是会直接load出8（该值为设定的cacheline大小）个字节填充整个cache line。</li></ul></li><li>Cache line size 的选择<ul><li>size指的是offset的大小，也就是Cache line的容量。</li><li>size增大，失效率一般会下降（更大程度的挖掘空间局部性）。</li><li>但是当Cache linesize占cache容量比例增加到一定程度，失效率会随之上升。原因如下：<ul><li>上面所描述的情况会导致Cache中可存放的cacheline数变少了，会导致数据经常被挤走。</li><li>cacheline的size不断增大，导致各字之间的空间局部性也会降低，失效率的收益不断减小。</li><li>失效损失增大，失效损失主要指从主存中读取数据并加载到cache的时间，size不断增大，导致传输时间也不断增大。</li></ul></li></ul></li></ul><h2 id="cache控制器怎么确定是否命中">2. Cache控制器怎么确定是否命中</h2><ul><li>首先假设cache 大小为64Bytes，cacheline大小为8Bytes。假设CPU想从地址为0x0654地址取一个字节数据。</li><li>考虑cacheline大小为8Bytes，使用地址低3bit来寻址其中的一个字节。(offset[2:0])</li><li>计算可得有8个cacheline，可以使用3bit地址来寻址是哪个cacheline。(index[5:3])</li><li>对于一个地址，即使确定了[5：3]bit，也不能就确信找到了正确的地址，还有更高位数信息。<ul><li>cache将高位信息用tag表示，tag、index、offset可以确定唯一的那个地址。</li></ul></li><li>检查是否命中：首先根据index找到cacheline，然后将cache的tag与地址的tag比对，如果相等，说明命中，如果不等，说明缺失。</li></ul><p><img src="3201119-20230609140440924-1848801620.png" /></p><ul><li>valid bit<ul><li>tag 前面还加了一位validbit，首先判断该位，判断缓存中的数据是否有效，若无效不用判断是否命中，直接缺失。</li><li>目的：考虑一些情况如：处理器刚启动时，cache中没有有效数据，对比是无用的。以及运行过程中，cache的一些cacheline可能还是空的，对比也是无用的。</li></ul></li><li>Cache 失效的处理<ul><li>这里以指令Cache失效为例。<ul><li>对于按序处理器，Cache失效，需要停顿流水线等待内存返回数据。<ul><li>对于乱序处理器，Cache失效，在等待cache失效处理时允许继续执行指令，更为复杂，这里暂不讨论。</li></ul></li><li>将PC值-4发送给内存。<ul><li>考虑到程序计数器是在执行的第一个时钟周期递增的，所以引发指令Cache失效的指令地址等于当前PC-4。</li></ul></li><li>对主存进行读操作，等待主存完成本次访问。</li><li>写Cacheline，将内存获取的数据写入，将高位填充到tag字段，将有效位拉高。</li><li>重启指令执行，重新取指，此次取指将会在指令cache中命中。</li></ul></li><li>数据Cache控制本质和上面相同，失效也需要短暂的暂停处理器，直到内存返回数据给Cache。</li></ul></li></ul><h2 id="直接映射缓存的优缺点">3. 直接映射缓存的优缺点</h2><ul><li>优点：硬件设计更简单、成本低。</li><li>缺点：cache thrashing（cache 颠簸）<ul><li>与上面假设相同，此时cpu想依次访问0x00，0x40，0x80地址的数据，这三个对应的index和offset都是一个，唯一区别是tag值。</li><li>访问0x00时，cache缺失，cpu从主存中load出8个字节大小的数据填充cacheline。</li><li>访问0x40时，索引到第0行cacheline，但这里存的是0x00对应的数据，仍缺失，再加载0x40地址的数据。</li><li>依次类推，0x80也要经过刚才两步：缺失+从主存中load。</li><li>这样cache对性能没什么提升。</li></ul></li><li>为了解决这个问题，引入了多路组相连缓存。</li></ul><h2 id="两路组相连缓存">4. 两路组相连缓存</h2><ul><li>假设cache 大小为64Bytes，cacheline大小为8Bytes。假设CPU想从地址为0x0654地址取一个字节数据。<ul><li>一共有8个cache line，将一路改成两路4个cache line。</li><li>offset依然是3bit，index只需要2bit即可。</li><li>找到某行index，对应两个cache line。然后将两个cacheline对应的tag与地址部分的tag进行对比。</li></ul></li><li>下图中，Tag的比较和DataSRAM的数据读取是并行的，可以增加cache读取速度。<ul><li>也可以串行，先比较Tag，再根据Tag比较结果去访问Data SRAM。</li></ul></li></ul><p><img src="3201119-20230807213451864-968491639.png" /></p><ul><li>缺点<ul><li>硬件成本更高，每次比较tag要比较多个cacheline对应的tag。</li></ul></li><li>优点<ul><li>可以降低cache颠簸可能性。前面提到的问题，0x00和0x40可以被加载到不同的路中。</li></ul></li></ul><h2 id="全相连缓存">5. 全相连缓存</h2><ul><li>取消index信号，每个cacheline占一个way。其实就相当于不考虑offset的3bit，其余地址一个一个放在表中的一条一条上，在这个表里挨个查，显而易见只适合小容量的cache。</li><li>此时查找是否命中，需要将地址的tag与所有组的tag进行比较。</li><li>因为不存在index，任意数据可以存在任意位置，可以最大程度降低cache的颠簸性。</li><li>硬件成本最高。</li></ul>]]></content>
    
    
    <categories>
      
      <category>计算机组成与设计</category>
      
      <category>Cache</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>关于Cache</title>
    <link href="/2023/10/29/%E5%85%B3%E4%BA%8ECache/"/>
    <url>/2023/10/29/%E5%85%B3%E4%BA%8ECache/</url>
    
    <content type="html"><![CDATA[<ul><li>参考<ul><li><a href="https://zhuanlan.zhihu.com/p/102293437"class="uri">https://zhuanlan.zhihu.com/p/102293437</a></li><li><a href="https://zhuanlan.zhihu.com/p/102326184"class="uri">https://zhuanlan.zhihu.com/p/102326184</a></li><li>公众号：老秦谈芯</li></ul></li></ul><hr /><ul><li>后续有时间的情况下，可以阅读《计算机组成与设计-硬件软件接口》第五章的内容，对写的博客进行补充。</li></ul><hr /><h2 id="为什么需要cache">1. 为什么需要Cache</h2><ul><li>运行一个进程的步骤（假设为一个变量a加1）<ul><li>首先从磁盘（辅存）中读出可执行程序，并将其load到主存储器中。</li><li>CPU从主存储器中读出地址为A的数据发到CPU的通用寄存器中。</li><li>将通用寄存器的值加1.</li><li>CPU再将通用寄存器的值写给主存储器。</li></ul></li><li>上面的步骤中，第三步的速度很快，但是第二步和第四步，与主存的交互很慢。为了解决这个问题，使用一块速度极快但是容量小的存储设备：cachemeomory。</li></ul><p><img src="3201119-20230609101424628-456798622.png" /></p><ul><li>将Cache放在CPU和主存之间，作为主存数据的缓存，当CPU想从主存中取数据会首先检查Cache中是否有对应地址的数据，如果有的话就可以直接取出给CPU使用。</li></ul><h2 id="多级cache存储结构">2. 多级Cache存储结构</h2><ul><li>Cache不可避免的需要在容量和速度之间进行平衡，但是即使多级Cache的速度仍比主存要快的多.</li><li>以Cortex-A53为例，三级Cache分布如下<ul><li>每个CPU都有一个L1 Cache，L1Cache会分为单独的ICache（指令）和DCache（数据）。<ul><li>ICache和DCache本质是一样的，L1 Cache单独分开的原因：<ul><li>CPU执行时，可以同时从两个Cache中获取指令和数据，做到硬件上的并行，提升性能。</li><li>DCache不仅需要考虑读出还要考虑写入的问题；而ICache只会被读取。分开两个电路设计，为了更快！</li></ul></li><li>L1Cache是最接近处理器的，要求其与CPU有近似的速度，也就注定了其容量不能太大，一般使用SRAM实现。</li><li>L1Cache中，ICache和DCache的大小一般32-64KB，2-4个时钟周期访问时间。</li></ul></li><li>一个Cluster内所有的CPU都共享一个L2 Cache。<ul><li>L2Cache是指令和数据共享，速度可以比CPU慢一些，主要功能是尽量保存更多的内容。</li><li>一般也是SRAM实现，大小为256KB-2MB，10-20个时钟周期的访问时间。<ul><li>虽然与L1一样都是SRAM实现，但是L1的SRAM设计是为了速度进行优化，采用更复杂更大更多的晶体管，因此成本和功耗都增加了不少。</li></ul></li></ul></li><li>所有的Cluster之间共享L3 Cache。而L3 Cache通过总线与主存相连。<ul><li>在一些系统设计中，L3 cache及更高级采用的是DRAM设计，成本更低。</li><li>L3 Cache一般大小为8-80MB，20-50个时钟周期的访问时间。</li></ul></li></ul></li><li>注意：上面描述的多个cluster就有多个二级缓存，那么就牵扯到<strong>缓存一致性</strong>的问题，后面会介绍。</li></ul><h2 id="多级cache之间的配合">3. 多级Cache之间的配合</h2><ul><li>inclusive cache（一个地址的数据可以存在多级缓存中）<ul><li>CPU想获取主存某地址的数据时，首先先访问L1Cache，看是否命中，如果命中直接返回数据给GPU。</li><li>如果L1 Cache缺失，则会在L2Cache中继续寻找，如果找到，那么会把数据返回给L1和CPU.(返回给L1是为了下次可以在L1的时候就命中)。</li><li>同理，L2找不到去找L3，找到返回L1,L2，CPU。</li><li>如果L3也缺失，CPU只能去主存储器中找数据，找到返回给L1,L2,L3,CPU。</li></ul></li><li>exclusive cache：某一地址的数据只能存在于多级Cache中的一级。</li></ul><h2 id="cache对代码的影响">4. Cache对代码的影响</h2><ul><li>首先看两端代码片段 <figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs abnf">int arr[<span class="hljs-number">10</span>][<span class="hljs-number">128</span>]<span class="hljs-comment">;</span><br><br>for (i <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-comment">; i &lt; 10; i++)</span><br>        for (j <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-comment">; j &lt; 128; j++)</span><br>                arr[i][j] <span class="hljs-operator">=</span> <span class="hljs-number">1</span><span class="hljs-comment">;</span><br></code></pre></td></tr></table></figure> <figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs abnf">int arr[<span class="hljs-number">10</span>][<span class="hljs-number">128</span>]<span class="hljs-comment">;</span><br><br>for (i <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-comment">; i &lt; 128; i++)</span><br>        for (j <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-comment">; j &lt; 10; j++)</span><br>                arr[j][i] <span class="hljs-operator">=</span> <span class="hljs-number">1</span><span class="hljs-comment">;</span><br></code></pre></td></tr></table></figure></li><li>假设cache line的大小是64字节</li><li>首先分析第一段代码<ul><li>cache控制器发现arr[0][0]缺失，便从主存中取出arr[0][0]到arr[0][15]（int数据类型占4个字节）。对于arr[0][1]-[15]来说就命中了。</li><li>之后到arr[0][16]时，又缺失，便又会在主存那里load出16个数据，64个字节。</li><li>这种情况得命中率还是较高的。</li></ul></li><li>分析第二段代码<ul><li>cache控制器发现arr[0][0]缺失，便从主存中取出arr[0][0]到arr[0][15]，之后读第二个数据arr[1][0]，结果发现又缺失了，依次类推。</li><li>当访问到arr[0][1]时，需要考虑cache的大小，如果大于数组arr的大小，那么经过前面的一系列缓存，arr[0][0]-arr[9][15]都被缓存下来，与第一段代码差不多的命中率。</li><li>但是如果cache的大小小于数组的大小，那么第二段代码的命中率就没有第一段高，自然要消耗更多的时间从主存中load数据。</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>计算机组成与设计</category>
      
      <category>Cache</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>FIR &amp; IIR 滤波器</title>
    <link href="/2023/10/28/FIR%20&amp;%20IIR%20%E6%BB%A4%E6%B3%A2%E5%99%A8/"/>
    <url>/2023/10/28/FIR%20&amp;%20IIR%20%E6%BB%A4%E6%B3%A2%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<ul><li>参考<ul><li><a href="https://www.cnblogs.com/alifpga/p/7902759.html"class="uri">https://www.cnblogs.com/alifpga/p/7902759.html</a></li><li><a href="https://www.zhihu.com/question/323353814"class="uri">https://www.zhihu.com/question/323353814</a></li></ul></li></ul><hr /><h2 id="fir-物理意义">1. FIR 物理意义</h2><ul><li>滤波，就是输入信号频率 X(f) 和期望的频率特征函数 H(f)进行相乘；这是在频域的计算。那么在时间域，是做了一个卷积的计算。</li><li>因此FIR做的就是将各个时刻的输入和对应的权重参数相乘，并叠加之后输出。<ul><li>如下图所示，为一N点的FIR滤波器。满足$ y(n) =_{k=0}^{N-1}h(k)x(n-k)$</li></ul></li></ul><p><img src="3201119-20230718173257360-1137724047.png" /></p><ul><li>由于FIR每一时刻的输出都取决于之前有限个输入，因此是“有限冲激响应”。</li></ul><h2 id="iir-物理意义">2. IIR 物理意义</h2><ul><li>IIR 滤波器设计的基本方法<ul><li>先设计一个合适的模拟滤波器，然后利用复值映射把模拟滤波器变换成数字滤波器。</li></ul></li><li>模拟原型滤波器<ul><li>有 巴特沃斯滤波器、切比雪夫滤波器、贝塞尔滤波器、椭圆滤波器等。</li><li>之后有时间会进行更详细的学习。</li></ul></li><li>模拟滤波器到数字滤波器的变换<ul><li>主要有两种方法<ul><li>脉冲响应不变法：从时域响应出发，让数字滤波器的单位脉冲响应h(n)模仿模拟滤波器的单位冲激响应ha(t)，h(n)等于ha(t)的取样值。<br /></li><li>双线性变换法：从频率响应出发，让数字滤波器的频率响应逼近模拟滤波器的频率响应，进而求得数字滤波器得系统函数。</li></ul></li></ul></li><li>无限冲激响应的理解<ul><li>首先看IIR滤波器表达式: <span class="math display">\[y[n] = a_0x[n]+a_1x[n-1]+a_2x[n-2]+a_3x[n-3]+...+b_1y[n-1]+b_2y[n-2]+b_3y[n-3]+...\]</span><ul><li>可以看到该公式是存在一个递归关系的，本步的计算结果会作为下一步的输入，无限递归下去。</li></ul></li><li>由于IIR是由模拟滤波器变换得到的，以下面滤波器为例。<ul><li>当给输入一个电压值（给输入一个冲激信号），电容会被充电，但是当电压值取消后，电容的电荷会被逐渐放掉，但是理论上永远不会变成0，导致输入会在无限长的时间产生影响。</li></ul><img src="3201119-20230718224246063-1237397880.png" /></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>数字信号处理</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>I2S协议</title>
    <link href="/2023/10/28/I2S%E5%8D%8F%E8%AE%AE/"/>
    <url>/2023/10/28/I2S%E5%8D%8F%E8%AE%AE/</url>
    
    <content type="html"><![CDATA[<ul><li>参考<ul><li>NXP 《I2S bus specification》</li><li>NXP 《I2S在Kinetis上的应用 》</li><li><a href="https://www.python100.com/html/R62183SDTAU0.html"class="uri">https://www.python100.com/html/R62183SDTAU0.html</a></li><li><a href="https://blog.51cto.com/u_15459030/5225825"class="uri">https://blog.51cto.com/u_15459030/5225825</a></li></ul></li></ul><hr /><h2 id="i2s概述">1. I2S概述</h2><h3 id="为什么需要i2s">1.1 为什么需要I2S</h3><ul><li>I2S是I2C的变种，全称：InterIc-Sound.专门为传输音频数据而设计的。</li><li>I2S 相较于I2C和SPI有以下优点<ul><li>更低的延迟：由于I2S数据传输是连续的，不需要等待ACK信号的回复，I2S只需要使用WS和SCK信号进行数据的同步；响应速度更快。</li><li>更高的精度：I2C一次可以传输8位的数据，但是I2S可以传输16/24位的数据，对于高精度的音频设备，I2S能够满足要求。</li></ul></li></ul><h3 id="i2s-三条总线">1.2 I2S 三条总线</h3><ul><li><p>之前介绍I2C有两条总线:SCL/SDA。而I2S有三条总线如下。</p><ul><li>SCK(Continuous Serial Clock)：串行时钟，也称为位时钟BCLK。<ul><li>SCK的时钟频率 = 声道数 * 采样频率 * 采样位数。</li></ul></li><li>WS(Word Select)：字段（声道）选择信号，也称为帧时钟LRCK。<ul><li>其频率等于采样频率。</li><li>在<strong>I2S模式下</strong>：WS=1，表示传递的是右声道数据。WS=0，表示传递的是左声道数据。后面有介绍，其它模式不同。</li></ul></li><li>SD(Serial Data)：串行数据。</li></ul></li><li><p>控制器（Controller）产生SCK信号和WS信号，控制器可以是Transmitter也可以是Receiver，也可以是单独设计的控制模块。</p><p><img src="3201119-20230724112142940-41134477.png" /></p></li></ul><h3 id="i2s的三种操作模式">1.3 I2S的三种操作模式</h3><ul><li>图片来自TI的TLV320AIC3104的数据表，其中WCLK为LRCLK信号，BCLK为SCK信号。</li><li>无论哪种模式，串行数据都是以二进制补码进行传输的，且先传输MSB，LSB的位置取决于I2S的位宽，长会被截断，短会被补零。</li><li>根据下面的波形可以看到：I2S模式，WS=0-&gt;左，WS=1-&gt;右;而左对齐和右对齐和I2S模式规定相反。</li><li>不同的模式决定了解码方式也不同。</li><li>根据SD和WCLK情况可分为三种模式：<ul><li><p>I2S模式</p><ul><li>在WCLK下降沿之后的一个BCLK周期的上升沿采到的数据有效。</li><li>WCLK在BCLK下降沿变化，发送方在BCLK下降沿改变数据，而接收方在BCLK上升沿采样数据。</li></ul><p><img src="3201119-20230724144906940-1187645121.png" /></p></li><li><p>左对齐模式</p><ul><li>相较于I2S模式，没有延迟一个BCLK周期。</li><li>不需要关心数据的长度，只会对LSB进行处理，截取/补零；但是MSB不会有问题。</li><li>发送方在BCLK下降沿改变数据，而接收方在BCLK上升沿采样数据。</li></ul><p><img src="3201119-20230724145852016-1096610293.png" /></p></li><li><p>右对齐模式</p><ul><li>不足：接收设备需要事先知道传输数据的长度，否则可能会导致MSB被截断</li></ul><p><img src="3201119-20230724162951792-210572944.png" /></p></li></ul></li><li>对于I2S模式和左对齐模式，可以允许发送端和接收端数据长度不同，因为接收端和发送端可以进行相应的截断和补0.<ul><li>为了保证数据音频信号的正确传输，发送端和接收端最好使用相同的数据格式和长度。</li></ul></li></ul><h2 id="i2s-应用">2. I2S 应用</h2><ul><li>注意：这里只讨论I2S模式，不讨论左对齐/右对齐模式。</li><li>讨论基于DMA和中断的乒乓缓冲区方案，旨在降低用于处理音频数据流的CPU开销。</li><li>为什么需要该方案<ul><li>采样率一般会在8KHz-48KHz之间，甚至可以更高。如果使用CPU去处理每个中断，那么系统效率会非常低。</li><li>另外大部分的音频算法会累积音频流中的数据形成缓冲数据块，之后再对缓存数据块进行处理。</li></ul></li></ul><h3 id="方案概述">2.1 方案概述</h3><ul><li>具体方案结构框图如下图所示。<ul><li>其中R和L分别代表右通道/左通道。每个通道都有用于乒乓操作的两个缓冲区（红色和黑色）。<ul><li>乒乓相关知识可以看<ahref="https://www.cnblogs.com/qianbinbin/p/17482594.html">这篇博客</a>。</li></ul></li><li>我对框图的工作模式理解是<ul><li>前面介绍，缓冲区被划分为四块；我们这里看红色和黑色两部分。红色部分我们使用DMA将其传送到I2STX模块。<ul><li>根据要求假设N为缓冲区中采样数据数量，DMA传送了N个采样数据之后，DMA会向CPU发起一个中断。</li></ul></li><li>中断期间，CPU执行音频解码算法之后获得的输出数据，并将输出数据填充到缓冲区。</li><li>可以看到存在一个乒乓的工作模式：<ul><li>T1：中断：CPU计算完成，将数据送到缓冲区R_TX0。与此同时DMA在搬移R_TX1的数据。</li><li>T2：DMA搬移结束后再次触发中断，此时DMA继续搬移R_TX0中的数据。CPU完成计算，并将数据送到缓冲区R_TX1中。</li><li>依次类推，形成乒乓的工作模式。</li></ul></li></ul></li><li>注意：考虑到音频信号具有较强的实时性要求，因此，所有计算都必须在下个中断发生之前完成，否则会导致系统故障。</li></ul></li></ul><p><img src="3201119-20230724200700025-1774790489.png" /></p><h3 id="i2s-的-fifo-特性">2.2 I2S 的 FIFO 特性</h3><ul><li>I2S 的 FIFO从DMA中读数据，FIFO中的数据会交替发送到左右通道；判断依据可选择FIFO中是否存在空数据。<ul><li>如下图所示，当FIFO的空数据计数为2时，就会让DMA加载一条数据进入左通道，一条数据进入右通道。</li></ul><img src="3201119-20230724220704113-1334935146.png" /></li></ul><h3 id="dma-和中断-配置">2.3 DMA 和中断 配置</h3><ul><li>2.1节中有介绍到DMA和中断CPU处理之间的乒乓工作模式。下面会依据实例详细介绍。</li><li>下图可以看到，共有四个数据块，每个数据块有四个采样，每个采样都有四个字节。<ul><li>DMA的访问顺序是左右通道交叉，如：0x00,0x20,0x04,0x24...</li></ul></li><li>乒乓操作实现<ul><li>当DMA读取地址经过0x00,0x20...到达0x2c时，TX0内的所有数据都已经发送完毕，发起一个中断。</li><li>随后，CPU进行算法计算，并将数据填充到BLOCK0和BLOCK2中；与此同时，DMA处理BLOCK1和BLOCK3。</li><li>当DMA到达地址0x3c时，会发起另一个中断。</li><li>随后，CPU进行算法计算，并将数据填充到BLOCK1和BLOCK3中；与此同时，DMA处理BLOCK0和BLOCK2。</li></ul></li></ul><div data-align="center"><img src="3201119-20230724221806028-635573909.png" width = 60%/></div>]]></content>
    
    
    <categories>
      
      <category>串行接口通信协议</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>I2C协议（一）</title>
    <link href="/2023/10/28/I2C%E5%8D%8F%E8%AE%AE%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <url>/2023/10/28/I2C%E5%8D%8F%E8%AE%AE%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<ul><li>参考<ul><li><a href="https://zhuanlan.zhihu.com/p/362287272"class="uri">https://zhuanlan.zhihu.com/p/362287272</a></li><li><a href="https://zhuanlan.zhihu.com/p/282949543"class="uri">https://zhuanlan.zhihu.com/p/282949543</a></li><li><ahref="https://blog.csdn.net/zhangduang_KHKW/article/details/121953275"class="uri">https://blog.csdn.net/zhangduang_KHKW/article/details/121953275</a></li></ul></li></ul><hr /><ul><li>波形文件来自NXP的IIC user manual.</li></ul><hr /><h2 id="i2c-用来做什么">1. I2C 用来做什么？</h2><ul><li>全称：Inter-Integrated Circuit.</li><li>一个双向，两线（SCL/SDA）制总线协议；用于主控器件和外围设备器件互联通信。简化PCB布线，降低成本。</li><li>I2C是一种多主机总线，所以也提供了仲裁功能，仲裁相关内容见<ahref="https://www.cnblogs.com/qianbinbin/p/17489279.html">这篇博客</a>。</li></ul><h2 id="i2c-的5种速率模式">2. I2C 的5种速率模式</h2><ul><li>对于不同的器件使用不同的模式，一共有5种模式，具体可看<ahref="https://zhuanlan.zhihu.com/p/362287272">这篇文章</a>.</li></ul><h2 id="通信过程">3. 通信过程</h2><ul><li>（1）当总线空闲时，SDA和SCL都处于高电平状态。</li><li>（2）当主设备决定开始通讯时，需要<strong>首先发送开始信号</strong>。</li><li>（3）发送从机设备的地址（7bits）以及1bit数据传送方向（R/W）；一共8bit，一个字节大小的数据。</li><li>（4）被寻址的从机发送应答信号给主机。</li><li>（5）发送器送出一个字节的数据，接收器收到完毕返回一个应答信号给主机。（发送器和接收器根据（3）中指定的传送方向分别选择为主机/从机）。</li><li>（6）重复（5）直到通信完成后，主机发送停止信号释放总线。</li><li>注意：<strong>发送数据过程中不可以改变数据传送方向，在（3）那步指定之后就不可更改。</strong>除非重启通信。</li></ul><h2 id="i2c的基础信号">4. I2C的基础信号</h2><ul><li><p>起始、停止、应答和非应答信号。</p><ul><li>起始信号：SCL处于高电平时，SDA从高电平到低电平变化，为起始信号。</li><li>停止信号：SCL处于高电平时，SDA从低电平到高电平变化，为停止信号。</li></ul><p><img src="3201119-20230618154435605-1791635155.png" /></p><ul><li>应答信号：其出现在一个字节传输完成之后，第9个SCL时钟周期内，SDA总线的控制权从主机给到从机，SDA总线由于上拉电阻的原因为高，如果从机正确的收到了数据，那么会将SDA拉低。<ul><li>主机发现SDA被拉低之后，可以选择下一步操作（发下一字节的传输/停止传输）。</li><li>需要注意，应答信号是接收设备给发送设备的反馈信号，而并不一定是从机给主机的反馈信号。</li></ul></li></ul><p><img src="3201119-20230618155923953-611387111.png" /></p><ul><li>非应答信号：第9个SCL时钟周期，SDA保持高电平，表示非应答，主机就需要发送停止信号，结束通信。以下情况可能会出现非应答：<ul><li>主机指定的地址，I2C总线上没有对应地址的从机设备。</li><li>主机发送从机地址，希望通信时；从机正忙，没办法通信。</li><li>主机接收从机发送的数据，主机产生非应答信号，告诉从机不要再发数据了，传输结束了。</li></ul></li></ul></li><li><p>数据有效性</p><ul><li>I2C在进行数据传送时，在SCL为低电平时发送器向SDA上送1bit数据，此时SDA可以发生变化；在SCL为高电平时，接收器从SDA上采样1bit数据，SDA需要保持稳定。</li></ul></li></ul><p><img src="3201119-20230618154320356-1776344245.png" /></p><ul><li>I2C传输的一帧有9位信号，包括一个字节的传输信号和1bit的应答/非应答信号。对于第一笔主机发送从机的传输包括（地址7bits+ 传输方向1bit）+ 1bit应答信号。<ul><li>对于1字节的数据，先发送高位，再传送低位。<br /></li></ul></li><li>前面介绍的SCL和SDA高低电平判断有些混乱，这里做一下总结。<ul><li>在开始和结束的判断，需要判断SCL信号为高电平时，SDA的变化。</li><li>在采样SDA时，需要判断SCL信号是否为高电平/低电平采样。</li><li>在判断应答/非应答，需要在SCL为高电平时，判断SDA信号是否被拉高/拉低。</li></ul></li></ul><hr /><p>如有问题，请指正！！</p>]]></content>
    
    
    <categories>
      
      <category>串行接口通信协议</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>I2C协议（二）</title>
    <link href="/2023/10/28/I2C%E5%8D%8F%E8%AE%AE%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <url>/2023/10/28/I2C%E5%8D%8F%E8%AE%AE%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<ul><li>参考<ul><li><ahref="https://blog.csdn.net/zhangduang_KHKW/article/details/121953275"class="uri">https://blog.csdn.net/zhangduang_KHKW/article/details/121953275</a></li><li><a href="https://blog.csdn.net/u010027547/article/details/47779975"class="uri">https://blog.csdn.net/u010027547/article/details/47779975</a></li><li><a href="https://blog.csdn.net/NeoZng/article/details/128486366"class="uri">https://blog.csdn.net/NeoZng/article/details/128486366</a></li><li><a href="https://www.cnblogs.com/DoreenLiu/p/14297191.html"class="uri">https://www.cnblogs.com/DoreenLiu/p/14297191.html</a></li><li><a href="https://zhuanlan.zhihu.com/p/388835566?utm_id=0"class="uri">https://zhuanlan.zhihu.com/p/388835566?utm_id=0</a></li></ul></li></ul><hr /><h2 id="仲裁机制">1. 仲裁机制</h2><ul><li>I2C是一个多主机的通信协议，那么就会出现多个主机都申请SDA总线权限发送开始信号和从机地址的情况，这就需要仲裁机制。</li></ul><h3 id="scl的同步">1.1 SCL的同步</h3><ul><li>这里以两个主机为例，可以拓展到多个主机上。目的：多个主机产生一个可公用的时钟。</li><li>两个主机的时钟频率不同，相位不同，使用时钟同步机制如下图所示：<ul><li>总线空闲时，SCL被上拉电阻拉高，当开始通信时，CLK1首先变成低电平，SCL也变成低电平，CLK2看到SCL拉低，便自己也拉低，并开始<strong>计数</strong>到CLK2虚线（原本应该拉低的位置）。</li><li>CLK2的低电平时间比CLK1要长，所以CLK1从拉高开始<strong>计数</strong>到CLK2和CLK1同时拉高（此时SCL也被拉高）的位置。</li><li>至此，CLK1获得了需要延长的低电平时间，CLK2获得了需要减短的高电平时间，都是通过前面计数得到的。CLK1和CLK2可以根据计数值进行调整，完成时钟的同步。</li></ul></li></ul><p><img src="3201119-20230618163225074-2122545585.png" /></p><h3 id="时钟扩展">1.2 时钟扩展</h3><ul><li>目的：I2C可以动态的调整总线的通信速率。</li><li>时钟扩展是从机发起的。<ul><li><ahref="https://www.cnblogs.com/qianbinbin/p/17488308.html">这篇博客</a>介绍了非应答信号，即从机正忙，没办法通信。SCL是正常由主机控制，在SCL为高电平时，主机采到第9个SCL周期，SDA为高，此时为非应答；主机就会发起停止信号，结束通信。</li><li>现在想要即使出现非应答也可以等待从机忙完之后可以继续传输的情况，可以对SCL信号做处理，如下。</li></ul></li><li>时钟扩展在基于上面描述，增加了从机可以控制SCL信号的硬件电路，从机在第9个SCL周期将SCL拉低，由于正忙，没办法接收处理传输的数据；主机检测到SCL被拉低，便会进入等待状态。等待从机忙完可以处理时，将SCL释放（I2C两条总线都接了上拉电阻，默认是高电平）并返回主机ACK信号，主机可以进行下一步的操作。</li></ul><h3 id="sda仲裁">1.3 SDA仲裁</h3><ul><li><strong>"线与逻辑"</strong>，在SCL为高电平时，SDA的数据等于各个主机的数据相与；每个主机将自己的数据和SDA上的数据进行比对，如果不一致，便知道失去了仲裁，就不再向SDA写数据。</li><li>如果两个主机都是向相同地址从机发起通信，那么仲裁继续，直到后续的数据位决定总线的归属权。</li><li>这种仲裁方式并不会破坏数据的有效性，因为总是有主机可以进行通信，且数据不会被改变。但这种仲裁方式无法提前设定主机的优先级。</li></ul><p><img src="3201119-20230618163641198-821963823.png" /></p><h2 id="读写时序">2. 读写时序</h2><ul><li>相同传送方向，不需要每次传输主机都发送开始信号。</li></ul><p><img src="3201119-20230618183803454-485534870.png" /></p><ul><li><p>主机写给从机</p><ul><li>其中器件地址为slave地址信息，而字地址为读/写从机内部存储单元的地址信息。</li></ul><p><img src="3201119-20230903220933713-114181872.png" /></p></li><li><p>主机读从机数据</p><ul><li><p>当前地址读</p><ul><li>I2C在读写操作之后，内部的地址指针自动加1，因此当前地址读不需要再次发送从机内部存储单元地址信息了。</li></ul><p><img src="3201119-20230903221220802-876809426.png" /></p></li><li><p>随机地址读</p><ul><li>如下图所示，需要先StartBit、发送slave地址，并设置传输为写方向；之后发送内部寄存器地址。这个过程称为DummyWrite.</li><li>之后再次重新发送StartBit、slave地址，并设置传输为读方向。从机应答，从机发送8bit数据。</li><li>为什么需要<strong>DummyWrite</strong>？因为是随机读，我们需要让从机存储单元的地址指针指向随机读取的地址，所以可以先进行DummyWrite，让存储单元的地址指针指向随机读的地址，等待从机应答后，就可以读取数据了。</li></ul></li></ul><p><img src="3201119-20230903213717676-1981068318.png" /></p></li><li><p>主机读后写/写后读</p><ul><li>传输过程中，改变传送方向，不需要主机发送停止信号释放总线，只需要重新发送开始信号就可以。</li></ul></li></ul><p><img src="3201119-20230618184525407-257373124.png" /></p>]]></content>
    
    
    <categories>
      
      <category>串行接口通信协议</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>串行接口通信协议-概述</title>
    <link href="/2023/10/28/%E4%B8%B2%E8%A1%8C%E6%8E%A5%E5%8F%A3%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE-%E6%A6%82%E8%BF%B0/"/>
    <url>/2023/10/28/%E4%B8%B2%E8%A1%8C%E6%8E%A5%E5%8F%A3%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE-%E6%A6%82%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<ul><li>参考<ul><li><a href="https://blog.csdn.net/Rocher_22/article/details/116590629"class="uri">https://blog.csdn.net/Rocher_22/article/details/116590629</a></li></ul></li></ul><hr /><h2 id="串行通信-并行通信">1. 串行通信 &amp; 并行通信</h2><ul><li>串行通信：利用一条传输线将数据位一位一位的传送。</li><li>并行通信：利用多条传输线将一个数据的多bit同时传送。</li><li>串行和并行哪个更快？<ul><li>在时钟频率较低时，因为并行可以同时传输多个bit，所以速率比串行要快。</li><li>时钟频率提高到一定程度时，由于并行通信存在很多平行且紧密的导线，信号变化越来越快，导致导线之间的干扰越来越严重。</li><li>串行通信导线少，且有差分信号加持，抗干扰能力更强，可以通过不断提升时钟频率来获得更高的传输速率，所以很多高速传输也使用串行通信，如USB、PCIe等。</li></ul></li></ul><h2 id="单工-半双工-全双工">2. 单工 &amp; 半双工 &amp; 全双工</h2><ul><li>串行通信按照传输的方向分类，有以下三种：<ul><li>单工：数据传输只能在一个固定方向上传输，这个方向固定后就不可更改，不能实现双向通信。</li><li>半双工：传输方向可以切换，但是在某个时刻，只允许数据在一个方向上传输。（如IIC通信）</li><li>全双工：允许数据同时两个方向传输，可以认为发送和接收是完全独立的。（如SPI通信）</li></ul></li></ul><h2 id="同步通信和异步通信">3.同步通信和异步通信</h2><ul><li>串行通信按照传输的方式分类，有以下两种：<ul><li>同步<ul><li>收发双方使用一根时钟信号，来进行双方数据同步，一般双方会规定在时钟上升/下降沿对数据进行采样。</li><li>SPI，IIC</li></ul></li><li>异步<ul><li>不使用时钟信号进行数据同步，而是<strong>在数据信号中穿插一些用于同步的信号位</strong>，或者以数据帧的格式传输数据，例如规定起始位、数据位、奇偶校验位、停止位等。</li><li>一些通讯还需要使用波特率衡量数据传送速率，以便更好的同步。</li><li>UART，但是USART可以同步&amp;异步通信。</li></ul></li><li>两者比较<ul><li>对于同步来说，传输内容大部分就是有效数据；而异步，传输内容会包含帧的各种标识符；所以同步通讯的效率更高。</li><li>但是同步对发送和接收方的时钟允许误差要求较小。</li></ul></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>串行接口通信协议</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>MIPI-CSI</title>
    <link href="/2023/10/28/MIPI-CSI/"/>
    <url>/2023/10/28/MIPI-CSI/</url>
    
    <content type="html"><![CDATA[<ul><li>参考<ul><li><ahref="https://blog.csdn.net/sinat_43629962/article/details/123089993"class="uri">https://blog.csdn.net/sinat_43629962/article/details/123089993</a></li><li><a href="https://zhuanlan.zhihu.com/p/599531271"class="uri">https://zhuanlan.zhihu.com/p/599531271</a></li><li><a href="https://www.mipi.org/"class="uri">https://www.mipi.org/</a></li></ul></li></ul><hr /><h2 id="csicamera-serial-interface">1. CSI（Camera SerialInterface）</h2><ul><li>观察下图，可以看到CSI是<strong>单向差分</strong>串行接口，传输数据和时钟信号。CCI(CameraControlInterface)是<strong>双线、双向、半双工的串行接口</strong>。数据传输协议符合I2C标准。I2C相关知识见<ahref="https://www.cnblogs.com/qianbinbin/p/17488308.html">这篇博客</a>。</li></ul><div data-align="center"><img src="3201119-20230903112723912-1950951905.png" width = 50%/></div><ul><li>CSI-2 layer Definitions<ul><li>如下图所示，CSI-2可以分为5层，分别为：应用层、组包/解包层，底层协议层（LowLevel Protocol）、通道管理层和物理层。下面对每一层进行解释。</li><li>PHY Layer：物理层<ul><li>该层定义了传输介质，输入/输出电路和时钟的机制，以便可以正确的从bitstream中捕获正确的0/1.</li><li>记录传输介质的电气参数特性以及数据和时钟之间的时序关系，记录了传输的起始位(SoT)和终止位(EoT)。</li></ul></li><li>Protocal Layer：协议层<ul><li>Pixel2Byte/Byte2Pixel Packing/Unpacking Layer：像素/字节转换层。<ul><li>Transmitter在将从Application接收到的数据发送前需要将像素数据转换为对应的字节流。Receiver在将数据提供给Application之前，需要将字节流数据转换为像素数据。</li></ul></li><li>Low Level Protocol(LLP)：底层协议层<ul><li>指的是SoT与EoT之间的数据包字节流协议，最小单元是字节。</li></ul></li><li>Lane Management：Lane管理层<ul><li>Lane是指一对差分数据对，在上面Figure1中，有N对差分数据，所以有N个Lane。</li><li>在Transimitter端，需要把上层打包好的数据按顺序依次Distribute到不同的lane上。</li><li>而在Reciever端需要从不同的lane上依次接收数据并把其按顺序Merge起来。</li></ul></li><li>总结<ul><li>在Transmitter端，从Apllication层接收到的数据，被打包成字节流数据包，可以选择将错误检查信息附加在要发送的数据流上，在底层协议层进行传输，并在LaneManagement层进行分发到不同的Lane上。</li><li>在Reciever端，从PHY层接收到数据后，经过LaneManagement层进行按序Merge，在底层协议层传输时，可以将Transmitter附加的信息剥离下来，并通过相对应的逻辑进行解释，检查数据发送的完整性和正确性。</li></ul></li></ul></li><li>Application layer<ul><li>该层主要用于不同场景对数据进行处理的过程，对于Transimitter，一般为Camera生成数据包；对于Reciever，多为SOC对数据进行处理。</li></ul></li></ul></li></ul><div data-align="center"><img src="3201119-20230903115043490-1797678616.png" width = 50%/></div><h2 id="ccicamera-control-interface">2. CCI（Camera ControlInterface）</h2><h3 id="cci-主从机定义">2.1 CCI 主从机定义</h3><ul><li>I2C支持多主机多从机传输，但CCI只支持一个主机的传输。</li><li>CCI将CSI的Transmitter配置为Slave，而将Reciever配置为Master。</li></ul><h3 id="cci-message-types">2.2 CCI Message Types</h3><ul><li>CCI传输信息类型包括：Start信号、ACK信号、Stop信号以及从机地址、从机内部寄存器子地址。<ul><li>CCI的从机地址位为7bit，8bit DATA传输，以及8bit/16bitINDEX传输。</li></ul></li></ul><div data-align="center"><img src="3201119-20230903161249778-1539658959.png" width = 50%/></div><h3 id="读写操作">2.3 读写操作</h3><ul><li><p>CCI支持四种读操作和两种写操作，下面展开介绍。</p></li><li><p>（1）随机位置单次读</p><ul><li>如下图所示，主机先会发起一个虚拟的写操作，指定好从机地址以及INDEX值后，再发起读操作。至于为什么需要DummyWrite可以看<ahref="https://www.cnblogs.com/qianbinbin/p/17489279.html">这篇博客</a>。</li><li>和I2C一样，更改数据传输方向需要再次发起开始信号Sr，并且主机需要再次给从机的地址，但不需要给INDEX值。</li><li>完成一次数据读操作之后，将SDA信号拉高表示主机不应答，结束传输。</li></ul><div data-align="center"><img src="3201119-20230903163440137-201009953.png" width = 50%/></div></li><li><p>（2）随机位置连续读</p><div data-align="center"><img src="3201119-20230903224825560-1446970425.png" width = 50%/></div></li><li><p>（3）当前位置单次读</p><ul><li>使用的是Previous_index + 1，不需要Dummy Write。</li></ul><div data-align="center"><img src="3201119-20230903223348145-837229902.png" width = 50%/></div></li><li><p>（4）当前位置连续读</p><div data-align="center"><img src="3201119-20230903224946740-1858592558.png" width = 50%/></div></li><li><p>（5）随机位置单次写</p><div data-align="center"><img src="3201119-20230903225103926-1703203642.png" width = 50%/></div></li><li><p>（6）随机位置连续写</p><ul><li>下图中，我认为最后一个Data对应的Index值应为M+L-1。</li></ul><div data-align="center"><img src="3201119-20230903225324858-1566777564.png" width = 50%/></div></li></ul><h3 id="cci-multi-byte-registers">2.4 CCI Multi-Byte Registers</h3><ul><li>CSI-2 协议支持以下寄存器宽度<ul><li>8-bits - 常用寄存器宽度</li><li>16bits - parameters like line-length, frame-length and exposurevalues（曝光值）</li><li>32bits - 用于高精度的寄存器宽度</li><li>64bits - for needs of future sensors</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>MIPI 接口协议</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>MIPI-概述</title>
    <link href="/2023/10/28/MIPI-%E6%A6%82%E8%BF%B0/"/>
    <url>/2023/10/28/MIPI-%E6%A6%82%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<ul><li>参考<ul><li><a href="https://zhuanlan.zhihu.com/p/92682047"class="uri">https://zhuanlan.zhihu.com/p/92682047</a></li></ul></li></ul><hr /><h2 id="关于-mipi">1. 关于 MIPI</h2><ul><li>MIPI：Mobile Industry ProcessorInterface，MIPI联盟发起的为移动应用处理器部分接口制定的开放标准。</li><li>MIPI 包含了一套协议和标准，MIPIalliance官网可以看到下面几种应用场景，都有相对应的协议。</li></ul><div data-align="center"><img src="3201119-20230902162549960-1373239895.png" width = 60%/></div><ul><li>以 Mobile System 为例，如下图。</li></ul><div data-align="center"><img src="3201119-20230902163024179-34270586.png" width = 40%/></div><h2 id="mipi-multimedia">2. MIPI Multimedia</h2><ul><li>下图为MIPIMultimedia涉及到的协议。分为三层：应用层、协议层和物理层。</li></ul><div data-align="center"><img src="3201119-20230902171954203-567302188.png" width = 50%/></div>]]></content>
    
    
    <categories>
      
      <category>MIPI 接口协议</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>硬件预取</title>
    <link href="/2023/10/28/%E7%A1%AC%E4%BB%B6%E9%A2%84%E5%8F%96/"/>
    <url>/2023/10/28/%E7%A1%AC%E4%BB%B6%E9%A2%84%E5%8F%96/</url>
    
    <content type="html"><![CDATA[<ul><li>参考<ul><li><a href="https://zhuanlan.zhihu.com/p/373038275"class="uri">https://zhuanlan.zhihu.com/p/373038275</a></li><li>《A Primer on Hardware Prefetching》</li><li><a href="http://home.ustc.edu.cn/~shaojiemike/posts/cache/"class="uri">http://home.ustc.edu.cn/~shaojiemike/posts/cache/</a></li><li><ahref="http://staff.ustc.edu.cn/~xhzhou/CA-Spring2020/chapter04-03.pdf"class="uri">http://staff.ustc.edu.cn/~xhzhou/CA-Spring2020/chapter04-03.pdf</a></li></ul></li></ul><hr /><h2 id="预取相关">1. 预取相关</h2><h3 id="为什么需要预取">1.1 为什么需要预取</h3><ul><li>Cache的设计可以一定程度上弥补CPU和内存之间的速度差距。</li><li>多级Cache的设计与两种类型的内存访问局限性相关，分别为：时间局域性和空间局域性。但局域性原理依赖于两个基本前提：<ul><li>（1）缓存的大小适合所有的工作负载和访问模式。<ul><li>但，工作负载的容量需求变化是高度动态的，不同工作负载下所适合的缓存层次和容量与速度之间的权衡各不同。</li></ul></li><li>（2）分配和替换缓存项的策略适用于所有工作负载和访问模式。<ul><li>但，内存访问模式也是高度动态的，所以很难说某个分配策略可以在所有情况下都表现良好。</li></ul></li></ul></li><li>考虑到上面的内容，Cache虽然可以一定程度上提升性能，但仍还需要其它的优化算法。</li></ul><h3 id="预取算法">1.2 预取算法</h3><ul><li>预取算法，是一种利用存储器的空闲带宽，提前预测取数据来隐藏内存访问延迟的方法。<ul><li>通过预测后续的内存访问，并提前完成该访问，（访问可能与CPU的其它不需要内存访问的操作并行进行），隐藏内存访问延时。</li><li>假设理想情况，预测都命中了，那么内存访问几乎不会造成任何延时开销；但实际上，预测并不总是及时/正确的。</li></ul></li><li>预测机制需要考虑以下因素<ul><li>内存访问的地址的预测</li><li>预测何时发出预取<ul><li>如果太早，会在暂存位置（这里假设是cache）保持一段时间，期间可能会挤走有用的cacheline，也可能会被其它挤走。</li><li>如果太晚，没有实现隐藏内存访问延迟的功能。</li></ul></li><li>选择合适的暂存 预取数据/地址 的位置</li></ul></li><li>预取实现方式<ul><li>硬件</li><li>软件</li><li>编译器</li></ul></li><li>评估指标<ul><li>覆盖率（coverage）<ul><li>预取命中占总访存比例。</li></ul></li><li>准确度（accuracy）<ul><li>所有预取中有效预取的占比。</li></ul></li></ul></li></ul><h3 id="预取值-暂存位置">1.3 预取值 暂存位置</h3><ul><li>根据预取值的暂存位置，可以分为绑定预取和非绑定预取。</li><li>绑定预取(binding prefetch)<ul><li>预取值直接加载到处理器的寄存器中。</li><li>在发起预取之后，寄存器的值将会被绑定。</li><li>缺点<ul><li>消耗了宝贵的寄存器资源。</li><li>在预取地址错误时，可能会导致程序语义错误。<br /></li></ul></li></ul></li><li>非绑定预取(non-binding prefetch)<ul><li>将预取值放入cache中，或是用于增强cache层次的补充缓冲区(supplementalbuffer)。</li><li>对于多核，预取值所在的cache或是buffer也需要满足cachecoherence，因此该部分地址空间的信息可能也会改变。</li><li>目前大部分处理器使用的都是非绑定预取策略。</li></ul></li></ul><h2 id="地址预测">2. 地址预测</h2><ul><li>如果没有准确的预测到地址，可能导致污染cache（将有效的cacheline内容挤出），以及消耗了额外的通信量和争用资源。</li><li>数据的地址预测<ul><li>主要考虑是对 独立变量还是数据结构元素或是其它数据类型的访问，以及数据在程序中执行的操作。<ul><li>举例：按顺序读取数组的每一个元素等，这种数据结构和操作方式很好预测。</li><li>举例：对变量交错访问/多个数据结构之间的遍历访问等，这种预测较为复杂。</li></ul></li></ul></li><li>指令的地址预测<ul><li>主要考虑程序是 顺序执行 还是 执行分支（一些跳转指令）。</li></ul></li><li>Cache 层级对地址预测的影响<ul><li>在最高层，处理器和L1 Cache的接口包含了所有可以用来预测内存引用的信息，基于这些信息可以实现高精度的预取。</li><li>在较低的层级中，关于预测内存引用的信息会被过滤。例如高层cache命中后，相关的信息就不会传到低层cache，只能观察到来自高层缺失的信息。</li><li>因此地址预测还与cache块的放置策略和替换策略相关。</li></ul></li></ul><p>​</p>]]></content>
    
    
    <categories>
      
      <category>计算机组成与设计</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
